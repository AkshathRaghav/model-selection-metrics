{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksha/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-01-16 09:43:25.438514: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-16 09:43:25.438576: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-16 09:43:25.439407: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-16 09:43:25.445078: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-16 09:43:26.366402: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "from transformers import AutoModel, AutoTokenizer, AutoFeatureExtractor, AutoImageProcessor, AutoProcessor, AutoModelForImageClassification\n",
    "from transformers import AutoModelForDepthEstimation, AutoModelForImageClassification, AutoModelForVideoClassification, AutoModelForMaskedImageModeling, AutoModelForObjectDetection, AutoModelForImageSegmentation, AutoModelForImageToImage, AutoModelForSemanticSegmentation, AutoModelForInstanceSegmentation, AutoModelForUniversalSegmentation, AutoModelForZeroShotImageClassification, AutoModelForZeroShotObjectDetection\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoModelForMaskedLM, AutoModelForSeq2SeqLM, \n",
    "    AutoModelForSequenceClassification, AutoModelForMultipleChoice, \n",
    "    AutoModelForNextSentencePrediction, AutoModelForTokenClassification, \n",
    "    AutoModelForQuestionAnswering, AutoModelForTextEncoding\n",
    ")\n",
    "from src.components.LogME import LogME\n",
    "from src.components.LEEP import LEEP\n",
    "from src.components.NCE import NCE\n",
    "import os\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from PIL import Image\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "class Models: \n",
    "    def __init__(self): \n",
    "        self.tasks_all = [\n",
    "            \"Feature Extraction\",\n",
    "            \"Text-to-Image\",\n",
    "            \"Image-to-Text\",\n",
    "            \"Image-to-Video\",\n",
    "            \"Text-to-Video\",\n",
    "            \"Visual Question Answering\",\n",
    "            \"Document Question Answering\",\n",
    "            \"Graph Machine Learning\",\n",
    "            \"Text-to-3D\",\n",
    "            \"Image-to-3D\",\n",
    "            \"Depth Estimation\",\n",
    "            \"Image Classification\",\n",
    "            \"Object Detection\",\n",
    "            \"Image Segmentation\",\n",
    "            \"Image-to-Image\",\n",
    "            \"Unconditional Image Generation\",\n",
    "            \"Video Classification\",\n",
    "            \"Zero-Shot Image Classification\",\n",
    "            \"Mask Generation\",\n",
    "            \"Zero-Shot Object Detection\",\n",
    "            \"Text Classification\",\n",
    "            \"Token Classification\",\n",
    "            \"Table Question Answering\",\n",
    "            \"Question Answering\",\n",
    "            \"Zero-Shot Classification\",\n",
    "            \"Translation\",\n",
    "            \"Summarization\",\n",
    "            \"Conversational\",\n",
    "            \"Text Generation\",\n",
    "            \"Text2Text Generation\",\n",
    "            \"Fill-Mask\",\n",
    "            \"Sentence Similarity\",\n",
    "            \"Text-to-Speech\",\n",
    "            \"Text-to-Audio\",\n",
    "            \"Automatic Speech Recognition\",\n",
    "            \"Audio-to-Audio\",\n",
    "            \"Audio Classification\",\n",
    "            \"Voice Activity Detection\",\n",
    "            \"Tabular Classification\",\n",
    "            \"Tabular Regression\",\n",
    "            \"Reinforcement Learning\",\n",
    "            \"Robotics\"\n",
    "        ]\n",
    "        self.tasks_all = [t.lower().replace(' ', '-') for t in self.tasks_all]\n",
    "        self.task_models = self.create_task_model_mapping(self.load_json_records()) \n",
    "        self.idx = 0 \n",
    "\n",
    "    def __call__(self, task, n):\n",
    "        if task in self.task_models:\n",
    "            self.idx = n\n",
    "            return self.task_models[task][self.idx:self.idx+n]\n",
    "        else:\n",
    "            return []\n",
    "            \n",
    "    def create_task_model_mapping(self, json_records, truncate=False):\n",
    "        task_model_mapping = {}\n",
    "        checks = {} \n",
    "        for task_one in self.tasks_all: \n",
    "            task_model_mapping[task_one] = [] \n",
    "            checks[task_one] = []\n",
    "        \n",
    "\n",
    "        for record in json_records:\n",
    "            tasks = record[0].split(',')\n",
    "            model_identifier = record[1]  \n",
    "            base_model_identifier = record[2]\n",
    "            dataset = record[4]\n",
    "            metric = record[5]\n",
    "\n",
    "            for task in tasks:\n",
    "                if task in task_model_mapping:\n",
    "                    if model_identifier not in task_model_mapping[task]:\n",
    "                        if (model_identifier, base_model_identifier) not in checks[task]:\n",
    "                            checks[task].append((model_identifier, base_model_identifier))\n",
    "                            task_model_mapping[task].append([model_identifier, base_model_identifier, dataset, metric])\n",
    "\n",
    "        if truncate: \n",
    "            task_models = {} \n",
    "            for task, models in task_model_mapping.items():\n",
    "                if len(models) > 5:\n",
    "                    task_models[task] = models\n",
    "            return task_models\n",
    "\n",
    "        return task_model_mapping\n",
    "    \n",
    "    def load_json_records(self):\n",
    "        with open('mapping.json') as f: \n",
    "            records = json.load(f)\n",
    "        return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Benchmarker:\n",
    "    def __init__(self, models: Models, logme=True, ckpt=False, regression = False, auto_increment_if_failed = False, leep=False, nce=False, test = False):\n",
    "        self.models = models\n",
    "        self.checkpoint = ckpt\n",
    "        self.auto_increment_if_failed = auto_increment_if_failed\n",
    "        self.test = test\n",
    "        if self.test: \n",
    "            print('----------Testing Benchmarker----------')\n",
    "\n",
    "        if int(logme) + int(leep) + int(nce) != 1:\n",
    "            raise ValueError(\"ERROR: Only one of logme, leep, or nce can be True\") \n",
    "        \n",
    "        if logme:\n",
    "            self.benchmark_model = LogME(regression=regression) \n",
    "            self.regression = regression    \n",
    "            print(f'SUCCESS: LogME initialized with {\"\" if regression else \"no\"} regression')\n",
    "        elif leep: \n",
    "            self.benchmark_model = LEEP()\n",
    "            print('SUCCESS: LEEP initalized')\n",
    "        else: \n",
    "            self.benchmark_model = NCE()\n",
    "            print('SUCCESS: NCE initalized')\n",
    "        \n",
    "        self.extractor_groups = {\n",
    "            TextFeatureExtractor: [\n",
    "                'text-classification',\n",
    "                'token-classification',\n",
    "                'question-answering',\n",
    "                'zero-shot-classification',\n",
    "                'translation',\n",
    "                'summarization',\n",
    "                'conversational',\n",
    "                'text-generation',\n",
    "                'text2text-generation',\n",
    "                'fill-mask',\n",
    "                'sentence-similarity', \n",
    "                'feature-extraction'\n",
    "            ],\n",
    "            ImageFeatureExtractor: [\n",
    "                'image-classification',\n",
    "                'object-detection',\n",
    "                'image-segmentation', \n",
    "                'video-classification'\n",
    "            ],\n",
    "            \"Multimodal Not Supported\": [\n",
    "                'text-to-image',\n",
    "                'image-to-text', \n",
    "                'table-question-answering'\n",
    "            ],\n",
    "            AudioFeatureExtractor: [\n",
    "                'automatic-speech-recognition',\n",
    "                'audio-classification'\n",
    "            ],\n",
    "        }\n",
    "\n",
    "    def __call__(self, design_specs):\n",
    "        task = design_specs['task']\n",
    "        dataloader = design_specs['dataset']\n",
    "        if not isinstance(dataloader, DataLoader): \n",
    "            raise ValueError('Dataset should be a `DataLoader` class!!') \n",
    "        n = design_specs['n']\n",
    "        return self.benchmark_models(task, dataloader, n)\n",
    "\n",
    "    def benchmark_models(self, task, dataloader, n=5): \n",
    "        if self.checkpoint:\n",
    "            checkpoint_pth = f'./checkpoints/{task}_{str(dataloader)}' \n",
    "            try: \n",
    "                os.mkdir(checkpoint_pth)\n",
    "            except: \n",
    "                pass\n",
    "\n",
    "        extractor_group = [x for x in self.extractor_groups if task in self.extractor_groups[x]][0]\n",
    "        if type(extractor_group) == str: \n",
    "            raise ValueError(f'ERROR: {extractor_group}')\n",
    "        \n",
    "        print('TASK: ', task)\n",
    "\n",
    "        models = self.models(task, n)\n",
    "        \n",
    "        print('MODELS: ', [model[0] for model in models])\n",
    "\n",
    "        if len(models) == 0:\n",
    "            raise Error('ERROR: No models selected!')\n",
    "        \n",
    "        benchmarks = {}\n",
    "        \n",
    "        check = False\n",
    "        for model_name, base_model_identifier, _, _ in models: \n",
    "            \n",
    "            if check and self.auto_increment_if_failed: \n",
    "                self.models(task, 1)\n",
    "            \n",
    "            check = False\n",
    "            \n",
    "            print('----------------------------------------')\n",
    "            print('Model_name', model_name)\n",
    "            if self.checkpoint:\n",
    "                model_checkpoint_pth = checkpoint_pth + f\"/{model_name.replace('/', '_')}/\"\n",
    "                try: \n",
    "                    os.mkdir(model_checkpoint_pth)\n",
    "                except: \n",
    "                    pass \n",
    "            \n",
    "            use_checkpoint = False \n",
    "            if os.path.exists(model_checkpoint_pth+'/config.json'): \n",
    "                try: \n",
    "                    config = json.load(model_checkpoint_pth+'/config.json')\n",
    "                    use_checkpoint=True\n",
    "                except: \n",
    "                    use_checkpoint=False\n",
    "\n",
    "            if not self.test: \n",
    "                if not use_checkpoint:\n",
    "                    extractor = extractor_group(task, model_name)\n",
    "                    if not extractor.feature_extractor or not extractor.model:\n",
    "                        print(f'     ERROR: Loading {model_name}, aborting benchmark!')\n",
    "                        check = True \n",
    "                        continue \n",
    "                    else: \n",
    "                        pass\n",
    "                else: \n",
    "                    pass\n",
    "            print('     SUCCESS: Extractor Loaded')\n",
    "            if not self.test: \n",
    "                if not use_checkpoint: \n",
    "                    features, targets = self.extract_features_and_targets(extractor, dataloader, n)\n",
    "\n",
    "                    # try: \n",
    "                    #     features, targets = self.extract_features_and_targets(extractor, dataloader, n)\n",
    "                    # except ValueError as e: \n",
    "                    #     print('Error with Extracting: ', e)\n",
    "                    #     check = True \n",
    "                    #     continue    \n",
    "                else: \n",
    "                    features, targets = config['features_pth'], config['targets_pth']\n",
    "\n",
    "            print('     SUCCESS: Feature Extraction Complete.')\n",
    "            print('     NOTE: Starting fitting the benchmarker.')\n",
    "            if not self.test: \n",
    "                score = self.fit(features, targets)\n",
    "            else: \n",
    "                score = 1\n",
    "            print(f'     SUCCESS: Benchmarker completed with score: {score}.')\n",
    "            print('----------------------------------------')\n",
    "\n",
    "            benchmarks[model_name] = score\n",
    "            \n",
    "            if self.checkpoint:\n",
    "                np.save(model_checkpoint_pth+'/features.npy', features)\n",
    "                np.save(model_checkpoint_pth+'/targets.npy', targets)\n",
    "                config = { \n",
    "                    'model_name': model_name,\n",
    "                    'dataset_name': str(dataloader),\n",
    "                    'score': score,\n",
    "                    'features_pth': model_checkpoint_pth+'/features.npy',\n",
    "                    'targets_pth': model_checkpoint_pth+'/targets.npy'\n",
    "                }\n",
    "                with open(model_checkpoint_pth+'/config.json', 'w') as fp:\n",
    "                    json.dump(config, fp)\n",
    "            del use_checkpoint, features, targets\n",
    "        return benchmarks\n",
    "    \n",
    "    def extract_features_and_targets(self, extractor, dataloader, n):\n",
    "        print('     NOTE: Extracting Features and Targets')\n",
    "        dataloader.reset()\n",
    "        features_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        progress_bar = tqdm(total=dataloader.max_idx, desc='       Progress: ')\n",
    "\n",
    "        batch = dataloader() \n",
    "        while batch: \n",
    "            features, labels = extractor(batch)\n",
    "            \n",
    "            if not len(features) and not len(labels): \n",
    "                batch = dataloader()\n",
    "                continue\n",
    "\n",
    "            features_list.append(features)\n",
    "            if type(labels) != np.ndarray: \n",
    "                labels = np.array([labels])\n",
    "            labels_list.append(labels)\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            batch = dataloader()\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "        if len(features_list) < n//2: \n",
    "            raise ValueError('ERROR: Not enough features extracted! (Could be sampling_rate error for AudioModels).')\n",
    "\n",
    "        f = np.concatenate(features_list, axis=0)\n",
    "        y = np.concatenate([np.array(self.convert_labels_with_pandas(labels_list))], axis=0)\n",
    "\n",
    "        f = f.reshape(f.shape[0], -1)\n",
    "\n",
    "        if not self.regression:\n",
    "            y = y.astype(int)\n",
    "        else:\n",
    "            if len(y.shape) == 1:\n",
    "                y = y.reshape(-1, 1)\n",
    "\n",
    "        print('     NOTE: Features Shape: ', f.shape)\n",
    "        print('     NOTE: Labels Shape: ', y.shape)\n",
    "\n",
    "        return f, y\n",
    "    \n",
    "    def convert_labels_with_pandas(self, labels_list):\n",
    "        flat_labels = [label for sublist in labels_list for label in sublist]\n",
    "        labels_series = pd.Series(flat_labels)\n",
    "        labels_series = labels_series.astype('category').cat.codes\n",
    "        return labels_series.values\n",
    "            \n",
    "    def fit(self, features, labels):\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        self.benchmark_model.reset()\n",
    "        return self.benchmark_model.fit(features, labels)\n",
    "    \n",
    "    def is_regression(self): \n",
    "        if self.test: \n",
    "            print('Testing is_regression')\n",
    "            return False\n",
    "\n",
    "        if all(isinstance(label, (int, float)) for label in self.labels):\n",
    "            unique_labels = set(labels)\n",
    "            if len(unique_labels) > len(labels) * 0.1:  \n",
    "                return True\n",
    "\n",
    "            if all(isinstance(label, int) for label in labels):\n",
    "                return False\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureExtractor:\n",
    "    def __init__(self, task_name:str, model_name: str):\n",
    "        self.feature_extractor = self.load_feature_extractor(model_name)\n",
    "        self.model = self.load_model(task_name, model_name)\n",
    "\n",
    "    def load_feature_extractor(self, model_name):\n",
    "        warnings.simplefilter(\"error\")\n",
    "        classes = [AutoTokenizer, AutoFeatureExtractor] \n",
    "        for class_ in classes:\n",
    "            try:\n",
    "                return class_.from_pretrained(model_name)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            except OSError as e: \n",
    "                pass \n",
    "        return None\n",
    "\n",
    "\n",
    "    def load_model(self, task_name, model_name):\n",
    "        classes = {\n",
    "            \"text-classification\": [\n",
    "                AutoModelForSequenceClassification,\n",
    "            ],\n",
    "            \"token-classification\": [\n",
    "                AutoModelForTokenClassification,\n",
    "            ],\n",
    "            \"question-answering\": [\n",
    "                AutoModelForQuestionAnswering,\n",
    "            ],\n",
    "            \"zero-shot-classification\": [\n",
    "                AutoModelForSequenceClassification,\n",
    "            ],\n",
    "            \"translation\": [\n",
    "                AutoModelForSeq2SeqLM,\n",
    "            ],\n",
    "            \"summarization\": [\n",
    "                AutoModelForSeq2SeqLM,\n",
    "            ],\n",
    "            \"conversational\": [\n",
    "                AutoModelForCausalLM,\n",
    "            ],\n",
    "            \"text-generation\": [\n",
    "                AutoModelForCausalLM,\n",
    "            ],\n",
    "            \"text2text-generation\": [\n",
    "                AutoModelForSeq2SeqLM,\n",
    "            ],\n",
    "            \"fill-mask\": [\n",
    "                AutoModelForMaskedLM,\n",
    "            ],\n",
    "            \"sentence-similarity\": [\n",
    "                AutoModelForSequenceClassification,\n",
    "            ],\n",
    "            \"feature-extraction\": [\n",
    "                AutoModelForTextEncoding,\n",
    "            ]\n",
    "            }\n",
    "        warnings.simplefilter(\"error\")\n",
    "\n",
    "        for task in classes: \n",
    "            if task in task_name: \n",
    "                for model_class in classes[task]: \n",
    "                    try: \n",
    "                        return model_class.from_pretrained(model_name)\n",
    "                    except: \n",
    "                        pass\n",
    "        return None\n",
    "        \n",
    "    def __call__(self, text):\n",
    "        # Text is a tuple \n",
    "        if type(text) != tuple: \n",
    "            raise ValueError(\"Input is not a tuple\")\n",
    "        b = self.feature_extractor(text[0], return_tensors=\"pt\", padding='max_length', truncation=True)\n",
    "        with torch.no_grad(): \n",
    "            c = self.model(**b)\n",
    "        return c.logits.numpy(), text[1]\n",
    "\n",
    "class ImageFeatureExtractor:\n",
    "    def __init__(self, task_name:str, model_name: str):\n",
    "        self.feature_extractor = self.load_feature_extractor(model_name)\n",
    "        self.model = self.load_model(task_name, model_name)\n",
    "    \n",
    "    def load_feature_extractor(self, model_name): \n",
    "        warnings.simplefilter(\"error\")\n",
    "        try: \n",
    "            return AutoFeatureExtractor.from_pretrained(model_name)\n",
    "        except: \n",
    "            print('     NOTE: Switching AutoLoader')\n",
    "            try: \n",
    "                return AutoImageProcessor.from_pretrained(model_name)\n",
    "            except: \n",
    "                return None\n",
    "            \n",
    "    def load_model(self, task_name, model_name): \n",
    "        classes = {'depth-estimation': [AutoModelForDepthEstimation], \n",
    "            'image-classification': [AutoModelForImageClassification], \n",
    "            'video-classification': [AutoModelForVideoClassification], \n",
    "            'object-detection': [AutoModelForObjectDetection], \n",
    "            'image-segmentation': [AutoModelForImageSegmentation, AutoModelForUniversalSegmentation, AutoModelForSemanticSegmentation, AutoModelForInstanceSegmentation], \n",
    "            'zero-shot-image-classification': [AutoModelForZeroShotImageClassification]}\n",
    "        warnings.simplefilter(\"error\")\n",
    "\n",
    "        for task in classes: \n",
    "            if task in task_name: \n",
    "                for model_class in classes[task]: \n",
    "                    try: \n",
    "                        return model_class.from_pretrained(model_name)\n",
    "                    except: \n",
    "                        pass\n",
    "        return None\n",
    "                \n",
    "\n",
    "    def __call__(self, image):\n",
    "        # Image is a tuple here\n",
    "        if type(image) != tuple: \n",
    "            print(image)\n",
    "            raise ValueError(\"Input is not a tuple\")\n",
    "        b = self.feature_extractor(images=image[0], return_tensors=\"pt\")\n",
    "        with torch.no_grad(): \n",
    "            c = self.model(**b)\n",
    "        return c.logits.numpy(), image[1]\n",
    "\n",
    "class DataLoader: \n",
    "    def __init__(self): \n",
    "        pass \n",
    "\n",
    "class ImageDataLoader(DataLoader): \n",
    "    def __init__(self, inputs: list[list[str, str]], name, shuffle=False, n=None):\n",
    "        self.name=name\n",
    "        self.img_paths = [x[0] for x in inputs] \n",
    "        self.targets = [x[1] for x in inputs]\n",
    "        self.idx = 0\n",
    "        self.max_idx = n\n",
    "        self.len = len(self.targets)\n",
    "        self.shuffle = shuffle  \n",
    "        self.dataframe = self.convert_to_pd()\n",
    "\n",
    "    def __str__(self): \n",
    "        return self.name\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len  \n",
    "    \n",
    "    def convert_to_pd(self): \n",
    "        df = pd.DataFrame({'img_path': self.img_paths, 'target': self.targets})\n",
    "        if self.shuffle: \n",
    "            df = df.sample(frac=1).reset_index(drop=True)\n",
    "        return df\n",
    "    \n",
    "    def __call__(self): \n",
    "        self.idx += 1\n",
    "        if (self.idx >= self.len) or (self.max_idx and self.idx > self.max_idx):\n",
    "            print('     Dataloader exhausted!') \n",
    "            return None\n",
    "        row = self.dataframe.iloc[self.idx]\n",
    "        image = None\n",
    "        with Image.open(row['img_path']) as f: \n",
    "            image = np.array(f)\n",
    "        return (image, row['target'])\n",
    "    \n",
    "    def reset(self): \n",
    "        self.idx = 0   \n",
    "\n",
    "class TextDataLoader(DataLoader): \n",
    "    def __init__(self, inputs: list[list[str, str]], name, shuffle=False, n=None):\n",
    "        self.name=name\n",
    "        self.texts = [x[0] for x in inputs] \n",
    "        self.targets = [x[1] for x in inputs]\n",
    "        self.idx = 0\n",
    "        self.max_idx = n\n",
    "        self.len = len(self.targets)\n",
    "        self.shuffle = shuffle  \n",
    "        self.dataframe = self.convert_to_pd()\n",
    "    \n",
    "    def __str__(self): \n",
    "        return self.name\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len  \n",
    "    \n",
    "    def convert_to_pd(self): \n",
    "        df = pd.DataFrame({'input': self.texts, 'target': self.targets})\n",
    "        if self.shuffle: \n",
    "            df = df.sample(frac=1).reset_index(drop=True)\n",
    "        return df\n",
    "    \n",
    "    def __call__(self): \n",
    "        self.idx += 1\n",
    "        if (self.idx >= self.len) or (self.max_idx and self.idx > self.max_idx):\n",
    "            print('     Dataloader exhausted!') \n",
    "            return None\n",
    "        row = self.dataframe.iloc[self.idx]\n",
    "        return (row['input'], row['target'])\n",
    "    \n",
    "    def reset(self): \n",
    "        self.idx = 0   \n",
    "\n",
    "class AudioDataLoader(DataLoader):\n",
    "    def __init__(self, inputs: list[list[str, str]], name, type: str, normalized:bool, shuffle=False, n=None):\n",
    "        self.name=name\n",
    "        self.audio_paths = [x[0] for x in inputs] \n",
    "        self.targets = [x[1] for x in inputs]\n",
    "        self.type=type\n",
    "        self.normalized=normalized\n",
    "        self.idx = 0\n",
    "        self.max_idx = n\n",
    "        self.len = len(self.targets)\n",
    "        self.shuffle = shuffle  \n",
    "        self.dataframe = self.convert_to_pd()\n",
    "\n",
    "    def __str__(self): \n",
    "        return self.name\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len  \n",
    "    \n",
    "    def convert_to_pd(self): \n",
    "        df = pd.DataFrame({'audio_path': self.audio_paths, 'target': self.targets})\n",
    "        if self.shuffle: \n",
    "            df = df.sample(frac=1).reset_index(drop=True)\n",
    "        return df\n",
    "    \n",
    "    def __call__(self): \n",
    "        self.idx += 1\n",
    "        if (self.idx >= self.len) or (self.max_idx and self.idx > self.max_idx):\n",
    "            print('     Dataloader exhausted!') \n",
    "            return None\n",
    "        row = self.dataframe.iloc[self.idx]\n",
    "        return ((self.read(row['audio_path'], type=self.type, normalized=self.normalized)), row['target'])\n",
    "    \n",
    "    def reset(self): \n",
    "        self.idx = 0\n",
    "    \n",
    "    def read(self, filename, type='wav', normalized=False):\n",
    "      with open(filename, 'rb') as file1:\n",
    "          a = pydub.AudioSegment.from_file(file1, format=type)\n",
    "          y = np.array(a.get_array_of_samples())\n",
    "          if a.channels == 2:\n",
    "              y = y.reshape((-1, 2))\n",
    "          if normalized:\n",
    "              return a.frame_rate, np.float32(y) / 2**15\n",
    "          else:\n",
    "              return a.frame_rate, np.float32(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForAudioClassification\n",
    "\n",
    "class AudioFeatureExtractor: \n",
    "  def __init__(self, task_name:str, model_name: str):\n",
    "    self.feature_extractor = self.load_feature_extractor(model_name)\n",
    "    self.model = self.load_model(task_name, model_name)\n",
    "  \n",
    "  def load_feature_extractor(self, model_name): \n",
    "    return AutoFeatureExtractor.from_pretrained(model_name)\n",
    "  \n",
    "  def load_model(self, task_name, model_name):\n",
    "    warnings.simplefilter(\"error\")\n",
    "    classes = {'audio-classification': [AutoModelForAudioClassification, AutoModel]}\n",
    "    for task in classes: \n",
    "        if task in task_name: \n",
    "            for model_class in classes[task]: \n",
    "                try: \n",
    "                    return model_class.from_pretrained(model_name)\n",
    "                except: \n",
    "                    pass\n",
    "    return None\n",
    "\n",
    "  def __call__(self, audio):\n",
    "    # Image is a tuple here\n",
    "    if type(audio) != tuple: \n",
    "        print(audio)\n",
    "        raise ValueError(\"Input is not a tuple\") \n",
    "    try: \n",
    "      b = self.feature_extractor(audio[0][1], sampling_rate=audio[0][0], return_tensors=\"pt\")\n",
    "    except ValueError as e: \n",
    "      # Sampling Rate issue\n",
    "      return None, None\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        c = self.model(**b)\n",
    "    return c.logits.numpy(), audio[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aircraft_targets(): \n",
    "  with open(\"./assets/aircraft/images_family_train.txt\") as file: \n",
    "    file_list = file.read().splitlines() \n",
    "  return [[f'./assets/aircraft/images/{file.split()[0]}.jpg', \"-\".join(file.split()[1:])] for file in file_list]\n",
    "\n",
    "def load_tweet_classifier(): \n",
    "  df = pd.read_csv('./assets/tweet/Corona_NLP_train.csv', encoding='latin-1')\n",
    "  df = df[['OriginalTweet', 'Sentiment']]\n",
    "  return df.astype(str).values.tolist()\n",
    "\n",
    "def load_esc50_classifier(): \n",
    "  df = pd.read_csv('./assets/esc_50/esc50.csv', encoding='latin-1')\n",
    "  filenames, targets = df['filename'].tolist(), df['target'].tolist()\n",
    "  return [[f'./assets/esc_50/audio/{filename}', target] for filename, target in zip(filenames, targets)]\n",
    "  \n",
    "  \n",
    "dataloader = ImageDataLoader(load_aircraft_targets(), name='airplane', shuffle=True, n=1000)\n",
    "# dataloader = TextDataLoader(load_tweet_classifier(), name='tweets', shuffle=True, n=1000)\n",
    "# dataloader = AudioDataLoader(load_esc50_classifier(), name='esc_50', type='wav', normalized=False, shuffle=True, n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = Models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = { \n",
    "    'task': 'image-classification',\n",
    "    'dataset': dataloader,  \n",
    "    'n': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: LogME initialized with  regression\n"
     ]
    }
   ],
   "source": [
    "benchmarker = Benchmarker(models, ckpt=True, logme=True, regression=True, auto_increment_if_failed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK:  image-classification\n",
      "MODELS:  ['Intel/vit-base-patch16-224-int8-static', 'MazenAmria/swin-tiny-finetuned-cifar100', 'OpenGVLab/internimage_xl_1k_384']\n",
      "----------------------------------------\n",
      "Model_name Intel/vit-base-patch16-224-int8-static\n",
      "     NOTE: Switching AutoLoader\n",
      "     ERROR: Loading Intel/vit-base-patch16-224-int8-static, aborting benchmark!\n",
      "----------------------------------------\n",
      "Model_name MazenAmria/swin-tiny-finetuned-cifar100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     NOTE: Switching AutoLoader\n",
      "     SUCCESS: Extractor Loaded\n",
      "     NOTE: Extracting Features and Targets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       Progress:   0%|          | 0/1000 [01:41<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbenchmarker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 62\u001b[0m, in \u001b[0;36mBenchmarker.__call__\u001b[0;34m(self, design_specs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset should be a `DataLoader` class!!\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m     61\u001b[0m n \u001b[38;5;241m=\u001b[39m design_specs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbenchmark_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 126\u001b[0m, in \u001b[0;36mBenchmarker.benchmark_models\u001b[0;34m(self, task, dataloader, n)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest: \n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_checkpoint: \n\u001b[0;32m--> 126\u001b[0m         features, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features_and_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;66;03m# try: \u001b[39;00m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m#     features, targets = self.extract_features_and_targets(extractor, dataloader, n)\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;66;03m# except ValueError as e: \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;66;03m#     continue    \u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m    135\u001b[0m         features, targets \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures_pth\u001b[39m\u001b[38;5;124m'\u001b[39m], config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets_pth\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[14], line 173\u001b[0m, in \u001b[0;36mBenchmarker.extract_features_and_targets\u001b[0;34m(self, extractor, dataloader, n)\u001b[0m\n\u001b[1;32m    171\u001b[0m batch \u001b[38;5;241m=\u001b[39m dataloader() \n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m batch: \n\u001b[0;32m--> 173\u001b[0m     features, labels \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(features) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(labels): \n\u001b[1;32m    176\u001b[0m         batch \u001b[38;5;241m=\u001b[39m dataloader()\n",
      "Cell \u001b[0;32mIn[15], line 120\u001b[0m, in \u001b[0;36mImageFeatureExtractor.__call__\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    118\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor(images\u001b[38;5;241m=\u001b[39mimage[\u001b[38;5;241m0\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \n\u001b[0;32m--> 120\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mnumpy(), image[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/transformers/models/swin/modeling_swin.py:1200\u001b[0m, in \u001b[0;36mSwinForImageClassification.forward\u001b[0;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;124;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1200\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1208\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1210\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(pooled_output)\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/transformers/models/swin/modeling_swin.py:1007\u001b[0m, in \u001b[0;36mSwinModel.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1003\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdepths))\n\u001b[1;32m   1005\u001b[0m embedding_output, input_dimensions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos)\n\u001b[0;32m-> 1007\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1017\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(sequence_output)\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/transformers/models/swin/modeling_swin.py:832\u001b[0m, in \u001b[0;36mSwinEncoder.forward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, output_hidden_states, output_hidden_states_before_downsampling, always_partition, return_dict)\u001b[0m\n\u001b[1;32m    828\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    829\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, hidden_states, input_dimensions, layer_head_mask, output_attentions\n\u001b[1;32m    830\u001b[0m     )\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 832\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_partition\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    837\u001b[0m hidden_states_before_downsampling \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/transformers/models/swin/modeling_swin.py:757\u001b[0m, in \u001b[0;36mSwinStage.forward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, always_partition)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[1;32m    755\u001b[0m     layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_partition\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    763\u001b[0m hidden_states_before_downsampling \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/transformers/models/swin/modeling_swin.py:688\u001b[0m, in \u001b[0;36mSwinLayer.forward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, always_partition)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    686\u001b[0m     attn_mask \u001b[38;5;241m=\u001b[39m attn_mask\u001b[38;5;241m.\u001b[39mto(hidden_states_windows\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 688\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    694\u001b[0m attention_windows \u001b[38;5;241m=\u001b[39m attention_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, channels)\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/transformers/models/swin/modeling_swin.py:563\u001b[0m, in \u001b[0;36mSwinAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    558\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    561\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    562\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 563\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    565\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Workbench/model_selection/metrics/model-selection-metrics/venv/lib/python3.9/site-packages/transformers/models/swin/modeling_swin.py:486\u001b[0m, in \u001b[0;36mSwinSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    481\u001b[0m relative_position_bias \u001b[38;5;241m=\u001b[39m relative_position_bias\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    483\u001b[0m )\n\u001b[1;32m    485\u001b[0m relative_position_bias \u001b[38;5;241m=\u001b[39m relative_position_bias\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 486\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mattention_scores\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrelative_position_bias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask is (precomputed for all layers in SwinModel forward() function)\u001b[39;00m\n\u001b[1;32m    490\u001b[0m     mask_shape \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "benchmarker(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydub \n",
    "import numpy as np\n",
    "\n",
    "def read(filename, type='wav', normalized=False):\n",
    "    \"\"\"MP3 to numpy array\"\"\"\n",
    "    with open(filename, 'rb') as file1:\n",
    "        a = pydub.AudioSegment.from_file(file1, format=type)\n",
    "        y = np.array(a.get_array_of_samples())\n",
    "        if a.channels == 2:\n",
    "            y = y.reshape((-1, 2))\n",
    "        if normalized:\n",
    "            return a.frame_rate, np.float32(y) / 2**15\n",
    "        else:\n",
    "            return a.frame_rate, np.float32(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44100, array([292., 293., 290., ..., 112., 126., 126.], dtype=float32))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read('/home/aksha/Workbench/model_selection/metrics/model-selection-metrics/assets/esc_50/audio/1-137-A-32.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load('/home/aksha/Workbench/model_selection/metrics/model-selection-metrics/checkpoints/text-classification_tweets/ASCCCCCCCC_distilbert-base-chinese-amazon_zh_20000/features.npy')\n",
    "targets = np.load('/home/aksha/Workbench/model_selection/metrics/model-selection-metrics/checkpoints/text-classification_tweets/ASCCCCCCCC_distilbert-base-chinese-amazon_zh_20000/targets.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
